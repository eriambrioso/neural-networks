{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OLS-from-scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJfl60T7Gmj7"
      },
      "source": [
        "# Benchmarking with OLS\n",
        "First we create a class for using ordinary least squares to fit the model and to predict outputs for unknown inputs. Numpy has some great linear algebra functions that we can use to help us quickly find the beta value for minimizing the sum of squared errors. We create two helper functions: *gradientDescent* and *computeGradient*, to help us implement gradient descent with our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oilhAyJUD32E"
      },
      "source": [
        "# import our handy dandy libraries \n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import scale\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "\n",
        "# increase the width of boxes in the notebook file (this is only cosmetic)\n",
        "np.set_printoptions(linewidth=180)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AEvzDsQHiRs"
      },
      "source": [
        "class OrdinaryLeastSquaresGradient:\n",
        "        \n",
        "    # fit the model to the data\n",
        "    def fit(self, X, y, h, tolerance, maxIterations):\n",
        "        self.n = X.shape[0]\n",
        "        self.d = X.shape[1]\n",
        "        \n",
        "        # save the training data\n",
        "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
        "        \n",
        "        # save the training labels\n",
        "        self.outputs = y\n",
        "        \n",
        "        # find the beta values that minimize the sum of squared errors via gradient descent\n",
        "        X = self.data\n",
        "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y)\n",
        "        self.beta = self.gradientDescent(L, (self.d + 1) * [0], h, tolerance, maxIterations)\n",
        "                \n",
        "    # predict the output from testing data\n",
        "    def predict(self, X):\n",
        "        # initialize an empty matrix to store the predicted outputs\n",
        "        yPredicted = np.empty([X.shape[0],1])\n",
        "        \n",
        "        # append a column of ones at the beginning of X\n",
        "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
        "        \n",
        "        # apply the function f with the values of beta from the fit function to each testing datapoint (rows of X)\n",
        "        for row in range(X.shape[0]):\n",
        "            yPredicted[row] = self.beta @ X[row,]\n",
        "            \n",
        "        return yPredicted\n",
        "\n",
        "    # run gradient descent to minimize the loss function\n",
        "    def gradientDescent(self, f, x0, h, tolerance, maxIterations):\n",
        "        # set x equal to the initial guess\n",
        "        x = x0\n",
        "\n",
        "        # take up to maxIterationsa number of steps\n",
        "        for counter in range(maxIterations):\n",
        "            # update the gradient\n",
        "            gradient = self.computeGradient(f, x, h)\n",
        "\n",
        "            # stop if the norm of the gradient is near 0\n",
        "            if np.linalg.norm(gradient) < tolerance:\n",
        "                print('Gradient descent took', counter, 'iterations to converge')\n",
        "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "                # return the approximate critical value x\n",
        "                return x\n",
        "\n",
        "            # if we do not converge, print a message\n",
        "            elif counter == maxIterations-1:\n",
        "                print(\"Gradient descent failed\")\n",
        "                print('The gradient is', gradient)\n",
        "                # return x, sometimes it is still pretty good\n",
        "                return x\n",
        "\n",
        "            # take a step in the opposite direction as the gradient\n",
        "            x -= h*gradient\n",
        "\n",
        "            \n",
        "    # compute the gradient\n",
        "    def computeGradient(self, f, x, h):\n",
        "        n = len(x)\n",
        "        gradient = np.zeros(n)\n",
        "\n",
        "        for counter in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[counter] += h\n",
        "            gradient[counter] = (f(xUp) - f(x))/h\n",
        "\n",
        "        return gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqZCP_PDyOvU"
      },
      "source": [
        "# With Momentum\n",
        "We can improve our gradient descent by adding momentum. Momentum will imporve our algorithm by taking more direct (more informed) steps toward the minimum and so, we can converge with fewer iterations. This method introduces a hyperparameter B. After some hyperparameter testing (and googling) a value of B=0.9 was found to be an ideal value for momentum. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5NrQg7hQSB_"
      },
      "source": [
        "class OrdinaryLeastSquaresGradientMomentum:\n",
        "        \n",
        "    # fit the model to the data\n",
        "    def fit(self, X, y, h, tolerance, maxIterations):\n",
        "        self.n = X.shape[0]\n",
        "        self.d = X.shape[1]\n",
        "        \n",
        "        # save the training data\n",
        "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
        "        \n",
        "        # save the training labels\n",
        "        self.outputs = y\n",
        "        \n",
        "        # find the beta values that minimize the sum of squared errors via gradient descent\n",
        "        X = self.data\n",
        "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y)\n",
        "        self.beta = self.gradientDescent(L, (self.d + 1) * [0], h, tolerance, maxIterations)\n",
        "                \n",
        "    # predict the output from testing data\n",
        "    def predict(self, X):\n",
        "        # initialize an empty matrix to store the predicted outputs\n",
        "        yPredicted = np.empty([X.shape[0],1])\n",
        "        \n",
        "        # append a column of ones at the beginning of X\n",
        "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
        "        \n",
        "        # apply the function f with the values of beta from the fit function to each testing datapoint (rows of X)\n",
        "        for row in range(X.shape[0]):\n",
        "            yPredicted[row] = self.beta @ X[row,]\n",
        "            \n",
        "        return yPredicted\n",
        "\n",
        "    # run gradient descent to minimize the loss function\n",
        "    def gradientDescent(self, f, x0, h, tolerance, maxIterations):\n",
        "        # set x equal to the initial guess\n",
        "        x = x0\n",
        "        # set the value for momentum\n",
        "        B = 0.9\n",
        "\n",
        "        # vdL will be used with B for updating the values in x\n",
        "        # we initialize with zeros the same length of x\n",
        "        vdL = np.zeros(len(x))\n",
        "       \n",
        "\n",
        "        # take up to maxIterations number of steps\n",
        "        for counter in range(maxIterations):\n",
        "            # update the gradient\n",
        "            gradient = self.computeGradient(f, x, h)\n",
        "            #print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "            # stop if the norm of the gradient is near 0\n",
        "            if np.linalg.norm(gradient) < tolerance:\n",
        "                print('Gradient descent took', counter, 'iterations to converge')\n",
        "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "                # return the approximate critical value x\n",
        "                return x\n",
        "            # if we do not converge, print a message\n",
        "            elif counter == maxIterations-1:\n",
        "                print(\"Gradient descent failed\")\n",
        "                print('The gradient is', gradient)\n",
        "                # return x, sometimes it is still pretty good\n",
        "                return x\n",
        "\n",
        "            # take a step in the opposite direction as the gradient with momentum!!\n",
        "            vdL = B*vdL + (1-B)*gradient\n",
        "            x -= h*vdL\n",
        "              \n",
        "    # compute the gradient\n",
        "    def computeGradient(self, f, x, h):\n",
        "        n = len(x)\n",
        "        gradient = np.zeros(n)\n",
        "\n",
        "        for counter in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[counter] += h\n",
        "            gradient[counter] = (f(xUp) - f(x))/h\n",
        "\n",
        "        return gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHTG4gIp8vXb"
      },
      "source": [
        "# Testing on Mount Pleasant Dataset\n",
        "For this we use the [Mount Pleasant Data](http://www.hawkeslearning.com/Statistics/dis/datasets.html) which consists of features of homes and their sell values. First we run the without momentum and then we run with momentum. We can see that the number of iterations it takes to converage decreases when we add momentum. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-iFnzyC8yWP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "e6b8765d-d70a-4c3d-8262-a194a33c51dd"
      },
      "source": [
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "# using google colab this is how I imported the data\n",
        "# first mount the drive\n",
        "#drive.mount('/content/drive/')\n",
        "\n",
        "# then import the data from the csv file to an numpy array\n",
        "path = \"/content/sample_data/Mount_Pleasant_Real_Estate_Data.csv\"\n",
        "data = pandas.read_csv(path, sep=',').to_numpy()\n",
        "\n",
        "# if you are using jupyter notebooks you can use this to import the data\n",
        "\"\"\"\n",
        "data = pandas.read_csv('data/Mount_Pleasant_Real_Estate_data.csv', sep=',').to_numpy()\n",
        "print(f'The real estate data\\n{data}\\nDimensions: {data.shape}')\n",
        "\"\"\"\n",
        "\n",
        "X = np.array(data[:,2:], dtype=float)\n",
        "y = np.array(data[:,1], dtype=float)\n",
        "#print(f'y values\\n{y}')\n",
        "#print(f'X values\\n{X}')\n",
        "\n",
        "# split the data into training and test sets\n",
        "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.20, random_state = 1)\n",
        "\n",
        "# Note that data is scaled and normalized.\n",
        "trainX = normalize(trainX)\n",
        "testX = normalize(testX)\n",
        "\n",
        "trainX = scale(trainX)\n",
        "testX = scale(testX)\n",
        "\n",
        "print('****FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES \\n')\n",
        "start = time.time()\n",
        "# instantiate an OLS model\n",
        "model = OrdinaryLeastSquaresGradient()\n",
        "\n",
        "# fit the model to the training data (find the beta parameters)\n",
        "model.fit(trainX, trainY, h = 0.001, tolerance = 4, maxIterations = 100000)\n",
        "\n",
        "# return the predicted outputs for the datapoints in the training set\n",
        "trainPredictions = model.predict(trainX)\n",
        "\n",
        "# return the predicted outputs for the datapoints in the test set\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "end = time.time()\n",
        "total_time = end-start\n",
        "\n",
        "# print the coefficient of determination r^2\n",
        "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
        "\n",
        "# print quality metrics\n",
        "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
        "\n",
        "# print quality metrics\n",
        "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))\n",
        "\n",
        "print('Total time elapsed is', total_time, 'seconds\\n')\n",
        "\n",
        "print('****FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE WITH MOMENTUM \\n')\n",
        "start = time.time()\n",
        "# instantiate an OLS model\n",
        "model = OrdinaryLeastSquaresGradientMomentum()\n",
        "\n",
        "# fit the model to the training data (find the beta parameters)\n",
        "model.fit(trainX, trainY, h = 0.001, tolerance = 4, maxIterations = 100000)\n",
        "\n",
        "# return the predicted outputs for the datapoints in the training set\n",
        "trainPredictions = model.predict(trainX)\n",
        "# return the predicted outputs for the datapoints in the test set\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "end = time.time()\n",
        "total_time = end-start\n",
        "# print the coefficient of determination r^2\n",
        "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
        "\n",
        "# print quality metrics\n",
        "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
        "\n",
        "# print quality metrics\n",
        "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))\n",
        "print('Total time elapsed is', total_time, 'seconds\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES \n",
            "\n",
            "Gradient descent took 2504 iterations to converge\n",
            "The norm of the gradient is 3.7822102990306807\n",
            "\n",
            "The r^2 score is 0.8681363993799857\n",
            "The mean absolute error on the training set is 79729.73464217168\n",
            "The mean absolute error on the test set is 68401.06903389767\n",
            "Total time elapsed is 1.1345787048339844 seconds\n",
            "\n",
            "****FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE WITH MOMENTUM \n",
            "\n",
            "Gradient descent took 2409 iterations to converge\n",
            "The norm of the gradient is 3.813598474563796\n",
            "\n",
            "The r^2 score is 0.8681363993800073\n",
            "The mean absolute error on the training set is 79729.73509409843\n",
            "The mean absolute error on the test set is 68401.07156949563\n",
            "Total time elapsed is 1.2796263694763184 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibut3oKBviHa"
      },
      "source": [
        "# With Net Elastic Loss\n",
        "Class that implements an elastic net loss function using gradient descent with momentum for linear regression. This method introduces two hyperparameters: L1 and L2. These values combine the penalties of ridge regression and lasso to get a better loss function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHfzYjw2xGDN"
      },
      "source": [
        "class ElasticLeastSquaresGradient:\n",
        "        \n",
        "    # fit the model to the data\n",
        "    def fit(self, X, y, h, tolerance, maxIterations):\n",
        "        self.n = X.shape[0]\n",
        "        self.d = X.shape[1]\n",
        "        \n",
        "        # save the training data\n",
        "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
        "        \n",
        "        # save the training labels\n",
        "        self.outputs = y\n",
        "       \n",
        "        # find the beta values that minimize the sum of squared errors via gradient descent\n",
        "        X = self.data\n",
        "\n",
        "        # lambda 1 and 2 for ENR \n",
        "        L1= 0.1\n",
        "        L2= 0.1\n",
        "      \n",
        "        # lambda function implements elastic net regression\n",
        "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y)+ L1*np.sum(np.abs(np.array(beta)))+L2*(np.array(beta).T@np.array(beta))       \n",
        "        self.beta = self.gradientDescentMomentum(L, (self.d + 1) * [0], h, tolerance, maxIterations)\n",
        "                \n",
        "    # predict the output from testing data\n",
        "    def predict(self, X):\n",
        "        # initialize an empty matrix to store the predicted outputs\n",
        "        yPredicted = np.empty([X.shape[0],1])\n",
        "        \n",
        "        # append a column of ones at the beginning of X\n",
        "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
        "        \n",
        "        # apply the function f with the values of beta from the fit function to each testing datapoint (rows of X)\n",
        "        for row in range(X.shape[0]):\n",
        "            yPredicted[row] = self.beta @ X[row,]\n",
        "            \n",
        "        return yPredicted\n",
        "\n",
        "    # run gradient descent to minimize the loss function\n",
        "    def gradientDescentMomentum(self, f, x0, h, tolerance, maxIterations):\n",
        "        # set x equal to the initial guess\n",
        "        x = x0\n",
        "        vdL = np.zeros(len(x))\n",
        "\n",
        "        B=0.9\n",
        "      \n",
        "        # take up to maxIterations number of steps\n",
        "        for counter in range(maxIterations):\n",
        "            # update the gradient\n",
        "            gradient = self.computeGradient(f, x, h)\n",
        "           \n",
        "            # stop if the norm of the gradient is near 0\n",
        "            if np.linalg.norm(gradient) < tolerance:\n",
        "                print('Gradient descent took', counter, 'iterations to converge')\n",
        "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "                # return the approximate critical value x\n",
        "                return x\n",
        "            # if we do not converge, print a message\n",
        "            elif counter == maxIterations-1:\n",
        "                print(\"Gradient descent failed\")\n",
        "                print('The gradient is', gradient)\n",
        "                # return x, sometimes it is still pretty good\n",
        "                return x\n",
        "\n",
        "            # take a step in the opposite direction as the gradient\n",
        "            \n",
        "            vdL = B*vdL + (1-B)*gradient\n",
        "            x -= h*vdL\n",
        "          \n",
        "    \n",
        "    # compute the gradient\n",
        "    def computeGradient(self, f, x, h):\n",
        "        n = len(x)\n",
        "        gradient = np.zeros(n)\n",
        "\n",
        "        for counter in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[counter] += h\n",
        "            gradient[counter] = (f(xUp) - f(x))/h\n",
        "\n",
        "        return gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-7Fa2tI0Ww_"
      },
      "source": [
        "# Testing on Weather in Szeged Dataset\n",
        "Using the [Weather in Szeged dataset](https://www.kaggle.com/budincsevity/szeged-weather) and the net elastic loss function from problem 3, predict the temperature. The result using L1 = 0.1 and L2 = 0.1 is a high R^2 value of ~0.9902."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRHpy6EsxsfU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "f54d72c9-ac46-4153-8658-a1b81b259c1d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# first mount the drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# then import the data from the csv file to an numpy array\n",
        "path = \"/content/sample_data/weatherHistory.csv\"\n",
        "data = pandas.read_csv(path, sep=',').to_numpy()\n",
        "\n",
        "# if you are using jupyter notebooks you can use this to import the data\n",
        "\"\"\"\n",
        "data = pandas.read_csv('data/weatherHistory.csv', sep=',').to_numpy()\n",
        "print(f'The real estate data\\n{data}\\nDimensions: {data.shape}')\n",
        "\"\"\"\n",
        "X = np.array(data[1:,2:11], dtype=float)\n",
        "y = np.array(data[1:,1], dtype=float)\n",
        "\n",
        "#print(f'y values\\n{y}')\n",
        "#print(f'X values\\n{X}')\n",
        "\n",
        "# split the data into training, test, and validation sets\n",
        "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 2)\n",
        "(trainX, valX, trainY, valY) = train_test_split(trainX, trainY, test_size=0.25, random_state =1) #0.8*0.25=0.2\n",
        "\n",
        "# Note that data is scaled\n",
        "trainX = scale(trainX)\n",
        "testX = scale(testX)\n",
        "valX = scale(valX)\n",
        "\n",
        "print('****FOR THE GRADIENT-BASED WITH ELASTIC NET LOSS  \\n')\n",
        "start = time.time()\n",
        "# instantiate an OLS model\n",
        "model = ElasticLeastSquaresGradient()\n",
        "\n",
        "# fit the model to the training data (find the beta parameters)\n",
        "# should use a very small h value \n",
        "model.fit(trainX, trainY, h = 0.000001, tolerance = 0.0001, maxIterations = 100000)\n",
        "\n",
        "# return the predicted outputs for the datapoints in the training set\n",
        "trainPredictions = model.predict(trainX)\n",
        "\n",
        "# return the predicted outputs for the datapoints in the test set\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "end = time.time()\n",
        "total_time = end-start\n",
        "\n",
        "# print the coefficient of determination r^2\n",
        "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
        "\n",
        "# print quality metrics\n",
        "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
        "\n",
        "# print quality metrics\n",
        "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))\n",
        "print('Total time elapsed is', total_time, 'seconds\\n')\n",
        "\n",
        "# print quality metrics\n",
        "predictions = model.predict(valX)\n",
        "print('The mean absolute error on the validation set is', mean_absolute_error(valY, predictions))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "****FOR THE GRADIENT-BASED WITH ELASTIC NET LOSS  \n",
            "\n",
            "Gradient descent took 428 iterations to converge\n",
            "The norm of the gradient is 9.680032013893703e-05\n",
            "\n",
            "The r^2 score is 0.9902035781792737\n",
            "The mean absolute error on the training set is 0.722750053115553\n",
            "The mean absolute error on the test set is 0.726523739651633\n",
            "Total time elapsed is 5.874943256378174 seconds\n",
            "\n",
            "The mean absolute error on the validation set is 0.7138900372533635\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}